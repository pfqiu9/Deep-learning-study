{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6d54dde-2df7-4fb7-a712-22928bed0323",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T09:00:40.533201Z",
     "iopub.status.busy": "2022-08-21T09:00:40.532821Z",
     "iopub.status.idle": "2022-08-21T09:00:41.861869Z",
     "shell.execute_reply": "2022-08-21T09:00:41.860953Z",
     "shell.execute_reply.started": "2022-08-21T09:00:40.533167Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import paddle\n",
    "paddle.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc35d00-4b26-4c08-a77f-b18ed49d2dbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T09:00:41.864988Z",
     "iopub.status.busy": "2022-08-21T09:00:41.864353Z",
     "iopub.status.idle": "2022-08-21T09:00:42.062885Z",
     "shell.execute_reply": "2022-08-21T09:00:42.062085Z",
     "shell.execute_reply.started": "2022-08-21T09:00:41.864955Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[2, 6, 128], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [[[ 0.83344501, -1.21844459,  0.48055834, ...,  0.20295902,\n",
      "          -0.24485584,  0.63060206],\n",
      "         [ 0.02267257, -1.50108027,  0.92258799, ..., -0.56661445,\n",
      "          -0.15777314,  0.36035648],\n",
      "         [ 0.10309691, -0.62477779,  0.71399939, ..., -0.90896100,\n",
      "           0.35118568,  1.10827327],\n",
      "         [-0.28135449, -1.22320652,  0.58787996, ...,  0.04233040,\n",
      "           0.45569146,  0.88440514],\n",
      "         [ 0.93284696, -0.67029673,  0.18253788, ...,  0.07425120,\n",
      "           0.05858831,  1.09639645],\n",
      "         [ 0.81336921, -1.01010621,  0.89278758, ..., -0.03291705,\n",
      "           0.10923889,  0.28272694]],\n",
      "\n",
      "        [[ 1.28254485, -0.68442333,  0.47062826, ..., -0.49197811,\n",
      "          -0.76637751,  1.21772254],\n",
      "         [ 1.70308220, -0.29101193,  1.18233836, ...,  0.54455656,\n",
      "          -0.71247047,  0.82578307],\n",
      "         [ 1.38030028, -1.23640335,  0.07849841, ...,  0.14094420,\n",
      "          -0.34609523,  0.24381794],\n",
      "         [ 0.12894052, -0.83433974,  0.68119502, ...,  0.27084792,\n",
      "          -0.53287524,  0.89941180],\n",
      "         [ 1.05299640, -0.63968891,  0.17641956, ...,  0.56504351,\n",
      "          -0.47513071,  1.30241930],\n",
      "         [ 1.02287424, -1.12174571,  0.71005636, ...,  0.30535245,\n",
      "          -0.40539044,  1.33949614]]])\n"
     ]
    }
   ],
   "source": [
    "# First put on the official api call, forgive my laziness.\n",
    "import paddle\n",
    "from paddle.nn import Transformer\n",
    "\n",
    "# src: [batch_size, tgt_len, d_model]\n",
    "enc_input = paddle.rand((2, 4, 128))\n",
    "# tgt: [batch_size, src_len, d_model]\n",
    "dec_input = paddle.rand((2, 6, 128))\n",
    "# src_mask: [batch_size, n_head, src_len, src_len]\n",
    "enc_self_attn_mask = paddle.rand((2, 2, 4, 4))\n",
    "# tgt_mask: [batch_size, n_head, tgt_len, tgt_len]\n",
    "dec_self_attn_mask = paddle.rand((2, 2, 6, 6))\n",
    "# memory_mask: [batch_size, n_head, tgt_len, src_len]\n",
    "cross_attn_mask = paddle.rand((2, 2, 6, 4))\n",
    "transformer = Transformer(128, 2, 4, 4, 512)\n",
    "output = transformer(enc_input,\n",
    "                     dec_input,\n",
    "                     enc_self_attn_mask,\n",
    "                     dec_self_attn_mask,\n",
    "                     cross_attn_mask)  # [2, 6, 128]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48e63bb1-85c6-446a-bbbd-c9988837c212",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T09:00:42.064412Z",
     "iopub.status.busy": "2022-08-21T09:00:42.063952Z",
     "iopub.status.idle": "2022-08-21T09:00:42.068031Z",
     "shell.execute_reply": "2022-08-21T09:00:42.067383Z",
     "shell.execute_reply.started": "2022-08-21T09:00:42.064384Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bacaca8-8e54-47f8-a8f1-15086901474f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T09:00:42.069572Z",
     "iopub.status.busy": "2022-08-21T09:00:42.068974Z",
     "iopub.status.idle": "2022-08-21T09:00:42.074980Z",
     "shell.execute_reply": "2022-08-21T09:00:42.074331Z",
     "shell.execute_reply.started": "2022-08-21T09:00:42.069545Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Layer):\n",
    "   \n",
    "    def __init__(self, temp, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temp = temp\n",
    "        self.dropout = nn.Dropout(p=attn_dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        attn = paddle.matmul(q/self.temp, k, transpose_y=True)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn * mask\n",
    "        attn = self.dropout(F.softmax(attn, axis=-1))\n",
    "\n",
    "        output = paddle.matmul(attn, v)\n",
    "\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb7d39ca-357b-4969-b500-93a822788b55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T09:00:42.076448Z",
     "iopub.status.busy": "2022-08-21T09:00:42.075845Z",
     "iopub.status.idle": "2022-08-21T09:00:42.086187Z",
     "shell.execute_reply": "2022-08-21T09:00:42.085520Z",
     "shell.execute_reply.started": "2022-08-21T09:00:42.076424Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Layer):\n",
    "   \n",
    "    def __init__(self, n_head=8, d_model=512, d_k=None, d_v=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k, bias_attr=False)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k, bias_attr=False)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v, bias_attr=False)\n",
    "        self.fc   = nn.Linear(n_head * d_v, d_model, bias_attr=False)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temp= d_k**0.5)\n",
    "\n",
    "        self.dropout   = nn.Dropout(dropout)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_model, epsilon=1e-5) \n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "      \n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "\n",
    "        batch_size, len_q, len_k, len_v = q.shape[0], q.shape[1], k.shape[1], v.shape[1]\n",
    "\n",
    "        residual = q\n",
    "        \n",
    "        q = self.w_qs(q).reshape((batch_size, len_q, n_head, d_k))\n",
    "        k = self.w_ks(k).reshape((batch_size, len_k, n_head, d_k))\n",
    "        v = self.w_vs(v).reshape((batch_size, len_v, n_head, d_v))\n",
    "\n",
    "        \n",
    "        q, k, v = q.transpose([0, 2, 1, 3]), k.transpose([0, 2, 1, 3]), v.transpose([0, 2, 1, 3])\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        \n",
    "        q, attn = self.attention(q, k, v, mask=mask)\n",
    "        \n",
    "\n",
    "        q = q.transpose([0, 2, 1, 3]).reshape((batch_size, len_q, -1))\n",
    "        q = self.dropout(self.fc(q))\n",
    "\n",
    "        q += residual\n",
    "        q = self.layer_norm(q)\n",
    "\n",
    "        return q, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65ef060d-cbe8-4a0e-9711-32b161c6f671",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T09:00:42.087223Z",
     "iopub.status.busy": "2022-08-21T09:00:42.086996Z",
     "iopub.status.idle": "2022-08-21T09:00:42.092921Z",
     "shell.execute_reply": "2022-08-21T09:00:42.092253Z",
     "shell.execute_reply.started": "2022-08-21T09:00:42.087203Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionwiseForward(nn.Layer):\n",
    "    \n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_in, d_hid)\n",
    "        self.w_2 = nn.Linear(d_hid, d_in)\n",
    "        self.layer_norm = nn.LayerNorm(d_in, epsilon=1e-5) # d_in: 需规范化的shape  epsilon:指明在计算过程中是否添加较小的值到方差中以防止除零,paddle中一般都采用1e-5\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99211e74-74f2-4dd8-99be-2e60d55516d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T09:00:42.094375Z",
     "iopub.status.busy": "2022-08-21T09:00:42.093803Z",
     "iopub.status.idle": "2022-08-21T09:00:42.099192Z",
     "shell.execute_reply": "2022-08-21T09:00:42.098517Z",
     "shell.execute_reply.started": "2022-08-21T09:00:42.094350Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Layer):\n",
    "\n",
    "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn  = PositionwiseForward(d_model, d_inner, dropout=dropout)\n",
    "\n",
    "    def forward(self, enc_input, self_attn_mask=None):\n",
    "\n",
    "        enc_output, enc_self_attn = self.slf_attn(enc_input, enc_input, enc_input, mask=self_attn_mask)\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "\n",
    "        return enc_output, enc_self_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9288c05-e119-4b47-b03c-f9e0e3557e39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T09:00:42.100525Z",
     "iopub.status.busy": "2022-08-21T09:00:42.100023Z",
     "iopub.status.idle": "2022-08-21T09:00:42.106002Z",
     "shell.execute_reply": "2022-08-21T09:00:42.105359Z",
     "shell.execute_reply.started": "2022-08-21T09:00:42.100501Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Layer):\n",
    "\n",
    "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn  = PositionwiseForward(d_model, d_inner, dropout=dropout)\n",
    "    def forward(self, dec_input, enc_output, self_attn_mask=None, dec_enc_attn_mask=None):\n",
    "        dec_output, dec_self_attn = self.self_attn(dec_input, dec_input, dec_input, mask=self_attn_mask)\n",
    "        dec_output, dec_enc_attn = self.enc_attn(dec_output, enc_output, enc_output, mask=dec_enc_attn_mask)\n",
    "        dec_output = self.pos_ffn(dec_output)\n",
    "\n",
    "        return dec_output, dec_self_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb854834-8c7e-4e6f-b67a-7b2e5208a87c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T09:00:42.107227Z",
     "iopub.status.busy": "2022-08-21T09:00:42.106830Z",
     "iopub.status.idle": "2022-08-21T09:00:42.113835Z",
     "shell.execute_reply": "2022-08-21T09:00:42.113145Z",
     "shell.execute_reply.started": "2022-08-21T09:00:42.107203Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#I don't understand it entirely\n",
    "class PositionalEncoding(nn.Layer):\n",
    "\n",
    "    def __init__(self, d_hid, n_position=200):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.register_buffer('pos_table', self._get_sin_encoding_table(n_position, d_hid))\n",
    "\n",
    "    def _get_sin_encoding_table(self, n_position, d_hid):\n",
    "\n",
    "        def get_position_angle_vec(position):\n",
    "            return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n",
    "\n",
    "        sin_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "        sin_table[:, 0::2] = np.sin(sin_table[:, 0::2])  # dim 2i\n",
    "        sin_table[:, 1::2] = np.cos(sin_table[:, 1::2])  # dim 2i+1\n",
    "\n",
    "        return paddle.to_tensor(sin_table, dtype='float32').unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "  \n",
    "        return x + paddle.cast(self.pos_table[:, :x.shape[1]], dtype='float32').detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "433fe619-9177-4c9c-bab9-fddc818bc832",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T09:00:42.115246Z",
     "iopub.status.busy": "2022-08-21T09:00:42.114675Z",
     "iopub.status.idle": "2022-08-21T09:00:42.123086Z",
     "shell.execute_reply": "2022-08-21T09:00:42.122437Z",
     "shell.execute_reply.started": "2022-08-21T09:00:42.115222Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Layer):\n",
    "\n",
    "    def __init__(self, n_src_vocab=200, d_word_vec=20, n_layers=6, n_head=2, \n",
    "        d_k=10, d_v=10, d_model=20, d_inner=10, pad_idx= 0, dropout=0.1, n_position=200, emb_weight=None):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, sparse=True, padding_idx=pad_idx)\n",
    "\n",
    "        if emb_weight is not None:\n",
    "            self.src_word_emb.weight.set_value(emb_weight)\n",
    "            self.src_word_emb.stop_gradient=True\n",
    "\n",
    "        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)\n",
    "        self.dropout      = nn.Dropout(dropout)\n",
    "        self.layer_stack  = nn.LayerList(\n",
    "            [\n",
    "                EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(d_model, epsilon=1e-5)\n",
    "\n",
    "    def forward(self, src_seq, src_mask, return_attns=False):\n",
    "\n",
    "        enc_slf_attn_list = []\n",
    "        print(\"src_seq:\",src_seq.shape)\n",
    "    \n",
    "        enc_output = self.dropout(self.position_enc(self.src_word_emb(src_seq)))\n",
    "        enc_output = self.layer_norm(enc_output)\n",
    "\n",
    "        for enc_layer in self.layer_stack:\n",
    "            enc_output, enc_slf_attn = enc_layer(enc_output, self_attn_mask=src_mask)\n",
    "            enc_slf_attn_list += [enc_slf_attn] if return_attns else []\n",
    "\n",
    "        if return_attns:\n",
    "            return enc_output, enc_slf_attn_list\n",
    "        return enc_output, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20128025-4ce8-4eef-b54e-1a0756b660f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T09:00:42.126110Z",
     "iopub.status.busy": "2022-08-21T09:00:42.125791Z",
     "iopub.status.idle": "2022-08-21T09:00:42.134353Z",
     "shell.execute_reply": "2022-08-21T09:00:42.133684Z",
     "shell.execute_reply.started": "2022-08-21T09:00:42.126086Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Layer):\n",
    "    \n",
    "    def __init__(self, n_trg_vocab=200, d_word_vec=20, n_layers=6, n_head=2, d_k=10, d_v=10,\n",
    "        d_model=20, d_inner=10, pad_idx=0, dropout=0.1, n_position=200, emb_weight=None):\n",
    "\n",
    "        super().__init__()\n",
    "        self.trg_word_emb = nn.Embedding(n_trg_vocab, d_word_vec, padding_idx=pad_idx)\n",
    "        if emb_weight is not None:\n",
    "            self.trg_word_emb.weight.set_value(emb_weight)\n",
    "            self.trg_word_emb.stop_gradient=True\n",
    "        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)\n",
    "        self.dropout      = nn.Dropout(dropout)\n",
    "        self.layer_stack  = nn.LayerList([\n",
    "            DecoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.layer_norm = nn.LayerNorm(d_model, epsilon=1e-5)\n",
    "\n",
    "    def forward(self, trg_seq, trg_mask, enc_output, src_mask, return_attns=False):\n",
    "\n",
    "        dec_self_attn_list, dec_enc_attn_list = [], []\n",
    "\n",
    "        dec_output = self.dropout(self.position_enc(self.trg_word_emb(trg_seq)))\n",
    "        dec_output = self.layer_norm(dec_output)\n",
    "\n",
    "        for dec_layer in self.layer_stack:\n",
    "            dec_output, dec_self_attn, dec_enc_attn = dec_layer(\n",
    "                dec_output, enc_output, self_attn_mask=trg_mask, dec_enc_attn_mask=src_mask\n",
    "            )\n",
    "            dec_self_attn_list += [dec_self_attn] if return_attns else []\n",
    "            dec_enc_attn_list += [dec_enc_attn] if return_attns else []\n",
    "        if return_attns:\n",
    "            return dec_output, dec_self_attn_list, dec_enc_attn_list\n",
    "        return dec_output, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4cb5965-67fc-453b-a34f-bcb8ff3c23d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T09:00:42.135780Z",
     "iopub.status.busy": "2022-08-21T09:00:42.135200Z",
     "iopub.status.idle": "2022-08-21T09:00:42.150566Z",
     "shell.execute_reply": "2022-08-21T09:00:42.149848Z",
     "shell.execute_reply.started": "2022-08-21T09:00:42.135756Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pad_mask(seq, pad_idx):\n",
    "    return (seq != pad_idx).unsqueeze(-2)\n",
    "\n",
    "def get_subsquent_mask(seq):\n",
    "\n",
    "    batch_size, len_s = seq.shape[0], seq.shape[1]\n",
    "    subsequent_mask = (1 - paddle.triu(paddle.ones((1, len_s, len_s)), diagonal=1)) \n",
    "    return subsequent_mask\n",
    "\n",
    "class Transformer(nn.Layer):\n",
    "\n",
    "    def __init__(\n",
    "        self, n_src_vocab, n_trg_vocab, src_pad_idx=0, trg_pad_idx=0, \n",
    "        d_word_vec=512, d_model=512, d_inner=2048,\n",
    "        n_layers=6, n_head=8, d_k=64, d_v=64, dropout=0.1, n_position=200,\n",
    "        src_emb_weight=None, trg_emb_weight=None,\n",
    "        trg_emd_prj_weight_sharing=True, emb_src_trg_weight_sharing=True,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_pad_idx, self.trg_pad_idx = src_pad_idx, trg_pad_idx\n",
    "        self.encoder = Encoder(\n",
    "            n_src_vocab=n_src_vocab, pad_idx=src_pad_idx, d_word_vec=d_word_vec,\n",
    "            n_layers=n_layers, n_head=n_head, d_model=d_model, d_inner=d_inner,\n",
    "            d_k=d_k, d_v=d_v, dropout=dropout, n_position=n_position,\n",
    "            emb_weight=src_emb_weight)\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            n_trg_vocab=n_trg_vocab, pad_idx=trg_pad_idx, d_word_vec=d_word_vec,\n",
    "            n_layers=n_layers, n_head=n_head, d_model=d_model, d_inner=d_inner,\n",
    "            d_k=d_k, d_v=d_v, dropout=dropout, n_position=n_position,\n",
    "            emb_weight=trg_emb_weight)\n",
    "        \n",
    "        self.trg_word_prj = nn.Linear(d_model, n_trg_vocab, bias_attr=False, \n",
    "                                      weight_attr=nn.initializer.XavierUniform())\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim()>1:\n",
    "                print(p.shape)\n",
    "                nn.initializer.XavierUniform(p)\n",
    "\n",
    "        # 判断维度是否相等，残差链接的维度是相等的\n",
    "        assert d_model == d_word_vec, 'To facilitate the residual connections, the dimensions of all module outputs shall be the same'\n",
    "        \n",
    "        self.x_logit_scale = 1.\n",
    "\n",
    "        if trg_emd_prj_weight_sharing:\n",
    "            weight = self.decoder.trg_word_emb.weight.numpy()\n",
    "            weight = np.transpose(weight)\n",
    "            self.trg_word_prj.weight.set_value(weight)\n",
    "            self.x_logit_scale= (d_model ** -0.5)\n",
    "\n",
    "        if emb_src_trg_weight_sharing:\n",
    "            weight = self.decoder.trg_word_emb.weight.numpy()\n",
    "            self.encoder.src_word_emb.weight.set_value(weight)\n",
    "        \n",
    "    def forward(self, src_seq, trg_seq):\n",
    "        src_mask = get_pad_mask(src_seq, self.src_pad_idx)\n",
    "\n",
    "        trg_mask = get_pad_mask(trg_seq, self.trg_pad_idx).numpy().astype(bool) & get_subsquent_mask(trg_seq).numpy().astype(bool)\n",
    "        trg_mask = paddle.to_tensor(trg_mask)\n",
    "        print(\"trg_mask:\",trg_mask.shape)\n",
    "        enc_output, *_ = self.encoder(src_seq, src_mask)\n",
    "\n",
    "        print(\"trg_seq,enc_output:\",trg_seq.shape, enc_output.shape)\n",
    "        dec_output, *_ = self.decoder(trg_seq, trg_mask, enc_output, src_mask)\n",
    "\n",
    "        seq_logit= self.trg_word_prj(dec_output) * self.x_logit_scale\n",
    "        print(\"seq_logit:\",seq_logit.shape)\n",
    "\n",
    "        return seq_logit.reshape((-1, seq_logit.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53ddc8e9-d8e0-4948-b2da-91021e219357",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T09:00:42.152064Z",
     "iopub.status.busy": "2022-08-21T09:00:42.151453Z",
     "iopub.status.idle": "2022-08-21T09:00:42.155085Z",
     "shell.execute_reply": "2022-08-21T09:00:42.154466Z",
     "shell.execute_reply.started": "2022-08-21T09:00:42.152039Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d791d631-c2f7-46fc-b7af-fb33a3d41993",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-21T09:00:42.156419Z",
     "iopub.status.busy": "2022-08-21T09:00:42.155931Z",
     "iopub.status.idle": "2022-08-21T09:00:43.256946Z",
     "shell.execute_reply": "2022-08-21T09:00:43.256151Z",
     "shell.execute_reply.started": "2022-08-21T09:00:42.156393Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Tensor(shape=[3, 10], dtype=int64, place=CPUPlace, stop_gradient=True,\n",
      "       [[16, 89, 43, 31, 64, 79, 75, 41, 19, 47],\n",
      "        [40, 91, 60, 58, 48, 62, 80, 65, 10, 89],\n",
      "        [75, 62, 5 , 92, 8 , 81, 34, 65, 32, 77]])\n",
      "******************************\n",
      "[200, 512]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[192, 512]\n",
      "[512, 2048]\n",
      "[2048, 512]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[192, 512]\n",
      "[512, 2048]\n",
      "[2048, 512]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[192, 512]\n",
      "[512, 2048]\n",
      "[2048, 512]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[192, 512]\n",
      "[512, 2048]\n",
      "[2048, 512]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[192, 512]\n",
      "[512, 2048]\n",
      "[2048, 512]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[192, 512]\n",
      "[512, 2048]\n",
      "[2048, 512]\n",
      "[200, 512]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[192, 512]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[192, 512]\n",
      "[512, 2048]\n",
      "[2048, 512]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[192, 512]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[192, 512]\n",
      "[512, 2048]\n",
      "[2048, 512]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[192, 512]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[192, 512]\n",
      "[512, 2048]\n",
      "[2048, 512]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[192, 512]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[192, 512]\n",
      "[512, 2048]\n",
      "[2048, 512]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[192, 512]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[192, 512]\n",
      "[512, 2048]\n",
      "[2048, 512]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[192, 512]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[512, 192]\n",
      "[192, 512]\n",
      "[512, 2048]\n",
      "[2048, 512]\n",
      "[512, 200]\n",
      "src_seq: [3, 10]\n",
      "trg_mask: [3, 10, 10]\n",
      "src_seq: [3, 10]\n",
      "trg_seq,enc_output: [3, 10] [3, 10, 512]\n",
      "seq_logit: [3, 10, 200]\n",
      "******************************\n",
      "Tensor(shape=[30, 200], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [[ 0.        ,  0.07907028, -0.02305841, ..., -0.02559329,\n",
      "         -0.01405847, -0.03748092],\n",
      "        [ 0.        ,  0.06910846,  0.00035859, ..., -0.01490618,\n",
      "         -0.00001874, -0.04527290],\n",
      "        [ 0.        ,  0.04475064, -0.03047930, ...,  0.00554594,\n",
      "          0.00304699, -0.08345012],\n",
      "        ...,\n",
      "        [ 0.        ,  0.03158572, -0.01457849, ..., -0.09265338,\n",
      "          0.00688282, -0.03889346],\n",
      "        [ 0.        ,  0.05070481,  0.01972982, ..., -0.05793019,\n",
      "          0.02857922, -0.03196013],\n",
      "        [ 0.        ,  0.03629639, -0.00056134, ..., -0.03243209,\n",
      "          0.01792589, -0.02762970]])\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "test_data = paddle.to_tensor(100*np.random.random((3, 10)), dtype='int64')\n",
    "print(\"*\"*30)\n",
    "print(test_data)\n",
    "print(\"*\"*30)\n",
    "enc = Encoder()\n",
    "dec = Decoder()\n",
    "transformer = Transformer(n_head=3, n_layers=6, src_pad_idx=0, trg_pad_idx=0, n_src_vocab=200, n_trg_vocab=200)\n",
    "enc_output, *_ = enc(test_data, src_mask=None)\n",
    "\n",
    "\n",
    "dec(test_data, trg_mask=None, enc_output=enc_output, src_mask=None)\n",
    "t = transformer(test_data, test_data)\n",
    "print(\"*\"*30)\n",
    "print(t)\n",
    "print(\"*\"*30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('paddle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fe7d3f7d1226b8bbf96157d896389e1752df8ed36a3471dc7e5d96b82f363992"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
