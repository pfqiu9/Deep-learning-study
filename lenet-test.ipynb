{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom PIL import Image\n\nDATADIR = '/work/palm/PALM-Training400/PALM-Training400'\n# 文件名以N开头的是正常眼底图片，以P开头的是病变眼底图片\nfile1 = 'N0012.jpg'\nfile2 = 'P0095.jpg'\n\n# 读取图片\nimg1 = Image.open(os.path.join(DATADIR, file1))\nimg1 = np.array(img1)\nimg2 = Image.open(os.path.join(DATADIR, file2))\nimg2 = np.array(img2)\n\n# 画出读取的图片\nplt.figure(figsize=(16, 8))\nf = plt.subplot(121)\nf.set_title('Normal', fontsize=20)\nplt.imshow(img1)\nf = plt.subplot(122)\nf.set_title('PM', fontsize=20)\nplt.imshow(img2)\nplt.show()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport random\nimport numpy as np\nimport os\n\n# 对读入的图像数据进行预处理\ndef transform_img(img):\n    # 将图片尺寸缩放道 224x224\n    img = cv2.resize(img, (224, 224))\n    # 读入的图像数据格式是[H, W, C]\n    # 使用转置操作将其变成[C, H, W]\n    img = np.transpose(img, (2,0,1))\n    img = img.astype('float32')\n    # 将数据范围调整到[-1.0, 1.0]之间\n    img = img / 255.\n    img = img * 2.0 - 1.0\n    return img\n\n# 定义训练集数据读取器\ndef data_loader(datadir, batch_size=10, mode = 'train'):\n    # 将datadir目录下的文件列出来，每条文件都要读入\n    filenames = os.listdir(datadir)\n    def reader():\n        if mode == 'train':\n            # 训练时随机打乱数据顺序\n            random.shuffle(filenames)\n        batch_imgs = []\n        batch_labels = []\n        for name in filenames:\n            filepath = os.path.join(datadir, name)\n            img = cv2.imread(filepath)\n            img = transform_img(img)\n            if name[0] == 'H' or name[0] == 'N':\n                # H开头的文件名表示高度近似，N开头的文件名表示正常视力\n                # 高度近视和正常视力的样本，都不是病理性的，属于负样本，标签为0\n                label = 0\n            elif name[0] == 'P':\n                # P开头的是病理性近视，属于正样本，标签为1\n                label = 1\n            else:\n                raise('Not excepted file name')\n            # 每读取一个样本的数据，就将其放入数据列表中\n            batch_imgs.append(img)\n            batch_labels.append(label)\n            if len(batch_imgs) == batch_size:\n                # 当数据列表的长度等于batch_size的时候，\n                # 把这些数据当作一个mini-batch，并作为数据生成器的一个输出\n                imgs_array = np.array(batch_imgs).astype('float32')\n                labels_array = np.array(batch_labels).astype('float32').reshape(-1, 1)\n                yield imgs_array, labels_array\n                batch_imgs = []\n                batch_labels = []\n\n        if len(batch_imgs) > 0:\n            # 剩余样本数目不足一个batch_size的数据，一起打包成一个mini-batch\n            imgs_array = np.array(batch_imgs).astype('float32')\n            labels_array = np.array(batch_labels).astype('float32').reshape(-1, 1)\n            yield imgs_array, labels_array\n\n    return reader\n\n# 定义验证集数据读取器\ndef valid_data_loader(datadir, csvfile, batch_size=10, mode='valid'):\n    # 训练集读取时通过文件名来确定样本标签，验证集则通过csvfile来读取每个图片对应的标签\n    # 请查看解压后的验证集标签数据，观察csvfile文件里面所包含的内容\n    # csvfile文件所包含的内容格式如下，每一行代表一个样本，\n    # 其中第一列是图片id，第二列是文件名，第三列是图片标签，\n    # 第四列和第五列是Fovea的坐标，与分类任务无关\n    # ID,imgName,Label,Fovea_X,Fovea_Y\n    # 1,V0001.jpg,0,1157.74,1019.87\n    # 2,V0002.jpg,1,1285.82,1080.47\n    # 打开包含验证集标签的csvfile，并读入其中的内容\n    filelists = open(csvfile).readlines()\n    def reader():\n        batch_imgs = []\n        batch_labels = []\n        for line in filelists[1:]:\n            line = line.strip().split(',')\n            name = line[1]\n            label = int(line[2])\n            # 根据图片文件名加载图片，并对图像数据作预处理\n            filepath = os.path.join(datadir, name)\n            img = cv2.imread(filepath)\n            img = transform_img(img)\n            # 每读取一个样本的数据，就将其放入数据列表中\n            batch_imgs.append(img)\n            batch_labels.append(label)\n            if len(batch_imgs) == batch_size:\n                # 当数据列表的长度等于batch_size的时候，\n                # 把这些数据当作一个mini-batch，并作为数据生成器的一个输出\n                imgs_array = np.array(batch_imgs).astype('float32')\n                labels_array = np.array(batch_labels).astype('float32').reshape(-1, 1)\n                yield imgs_array, labels_array\n                batch_imgs = []\n                batch_labels = []\n\n        if len(batch_imgs) > 0:\n            # 剩余样本数目不足一个batch_size的数据，一起打包成一个mini-batch\n            imgs_array = np.array(batch_imgs).astype('float32')\n            labels_array = np.array(batch_labels).astype('float32').reshape(-1, 1)\n            yield imgs_array, labels_array\n\n    return reader\n# 查看数据形状\nDATADIR = '/home/aistudio/work/palm/PALM-Training400/PALM-Training400'\ntrain_loader = data_loader(DATADIR, \n                           batch_size=10, mode='train')\ndata_reader = train_loader()\ndata = next(data_reader)\ndata[0].shape, data[1].shape\n\neval_loader = data_loader(DATADIR, \n                           batch_size=10, mode='eval')\ndata_reader = eval_loader()\ndata = next(data_reader)\ndata[0].shape, data[1].shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n# LeNet 识别眼疾图片\nimport os\nimport random\nimport paddle\nimport numpy as np\n\nDATADIR = '/home/aistudio/work/palm/PALM-Training400/PALM-Training400'\nDATADIR2 = '/home/aistudio/work/palm/PALM-Validation400'\nCSVFILE = '/home/aistudio/labels.csv'\n# 设置迭代轮数\nEPOCH_NUM = 5\n\n# 定义训练过程\ndef train_pm(model, optimizer):\n    # 开启0号GPU训练\n    use_gpu = True\n    paddle.device.set_device('gpu:0') if use_gpu else paddle.device.set_device('cpu')\n\n    print('start training ... ')\n    model.train()\n    # 定义数据读取器，训练数据读取器和验证数据读取器\n    train_loader = data_loader(DATADIR, batch_size=10, mode='train')\n    valid_loader = valid_data_loader(DATADIR2, CSVFILE)\n    for epoch in range(EPOCH_NUM):\n        for batch_id, data in enumerate(train_loader()):\n            x_data, y_data = data\n            img = paddle.to_tensor(x_data)\n            label = paddle.to_tensor(y_data)\n            # 运行模型前向计算，得到预测值\n            logits = model(img)\n            loss = F.binary_cross_entropy_with_logits(logits, label)\n            avg_loss = paddle.mean(loss)\n\n            if batch_id % 20 == 0:\n                print(\"epoch: {}, batch_id: {}, loss is: {:.4f}\".format(epoch, batch_id, float(avg_loss.numpy())))\n            # 反向传播，更新权重，清除梯度\n            avg_loss.backward()\n            optimizer.step()\n            optimizer.clear_grad()\n\n        model.eval()\n        accuracies = []\n        losses = []\n        for batch_id, data in enumerate(valid_loader()):\n            x_data, y_data = data\n            img = paddle.to_tensor(x_data)\n            label = paddle.to_tensor(y_data)\n            # 运行模型前向计算，得到预测值\n            logits = model(img)\n            # 二分类，sigmoid计算后的结果以0.5为阈值分两个类别\n            # 计算sigmoid后的预测概率，进行loss计算\n            pred = F.sigmoid(logits)\n            loss = F.binary_cross_entropy_with_logits(logits, label)\n            # 计算预测概率小于0.5的类别\n            pred2 = pred * (-1.0) + 1.0\n            # 得到两个类别的预测概率，并沿第一个维度级联\n            pred = paddle.concat([pred2, pred], axis=1)\n            acc = paddle.metric.accuracy(pred, paddle.cast(label, dtype='int64'))\n\n            accuracies.append(acc.numpy())\n            losses.append(loss.numpy())\n        print(\"[validation] accuracy/loss: {:.4f}/{:.4f}\".format(np.mean(accuracies), np.mean(losses)))\n        model.train()\n\n        paddle.save(model.state_dict(), 'palm.pdparams')\n        paddle.save(optimizer.state_dict(), 'palm.pdopt')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 定义评估过程\ndef evaluation(model, params_file_path):\n\n    # 开启0号GPU预估\n    use_gpu = True\n    paddle.device.set_device('gpu:0') if use_gpu else paddle.device.set_device('cpu')\n\n    print('start evaluation .......')\n\n    #加载模型参数\n    model_state_dict = paddle.load(params_file_path)\n    model.load_dict(model_state_dict)\n\n    model.eval()\n    eval_loader = data_loader(DATADIR, \n                        batch_size=10, mode='eval')\n\n    acc_set = []\n    avg_loss_set = []\n    for batch_id, data in enumerate(eval_loader()):\n        x_data, y_data = data\n        img = paddle.to_tensor(x_data)\n        label = paddle.to_tensor(y_data)\n        y_data = y_data.astype(np.int64)\n        label_64 = paddle.to_tensor(y_data)\n        # 计算预测和精度\n        prediction, acc = model(img, label_64)\n        # 计算损失函数值\n        loss = F.binary_cross_entropy_with_logits(prediction, label)\n        avg_loss = paddle.mean(loss)\n        acc_set.append(float(acc.numpy()))\n        avg_loss_set.append(float(avg_loss.numpy()))\n    # 求平均精度\n    acc_val_mean = np.array(acc_set).mean()\n    avg_loss_val_mean = np.array(avg_loss_set).mean()\n\n    print('loss={:.4f}, acc={:.4f}'.format(avg_loss_val_mean, acc_val_mean))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 导入需要的包\nimport paddle\nimport numpy as np\nfrom paddle.nn import Conv2D, MaxPool2D, Linear, Dropout\nimport paddle.nn.functional as F\n\n# 定义 LeNet 网络结构\nclass LeNet(paddle.nn.Layer):\n    def __init__(self, num_classes=1):\n        super(LeNet, self).__init__()\n\n        # 创建卷积和池化层块，每个卷积层使用Sigmoid激活函数，后面跟着一个2x2的池化\n        self.conv1 = Conv2D(in_channels=3, out_channels=6, kernel_size=5)\n        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n        self.conv2 = Conv2D(in_channels=6, out_channels=16, kernel_size=5)\n        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n        # 创建第3个卷积层\n        self.conv3 = Conv2D(in_channels=16, out_channels=120, kernel_size=4)\n        # 创建全连接层，第一个全连接层的输出神经元个数为64\n        self.fc1 = Linear(in_features=300000, out_features=64)\n        # 第二个全连接层输出神经元个数为分类标签的类别数\n        self.fc2 = Linear(in_features=64, out_features=num_classes)\n\n    # 网络的前向计算过程\n    def forward(self, x, label=None):\n        x = self.conv1(x)\n        x = F.sigmoid(x)\n        x = self.max_pool1(x)\n        x = self.conv2(x)\n        x = F.sigmoid(x)\n        x = self.max_pool2(x)\n        x = self.conv3(x)\n        x = F.sigmoid(x)\n        x = paddle.reshape(x, [x.shape[0], -1])\n        x = self.fc1(x)\n        x = F.sigmoid(x)\n        x = self.fc2(x)\n        if label is not None:\n            acc = paddle.metric.accuracy(input=x, label=label)\n            return x, acc\n        else:\n            return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 创建模型\nmodel = LeNet(num_classes=1)\n# 启动训练过程\nopt = paddle.optimizer.Momentum(learning_rate=0.001, momentum=0.9, parameters=model.parameters())\ntrain_pm(model, optimizer=opt)\nevaluation(model, params_file_path=\"palm.pdparams\")","metadata":{},"execution_count":null,"outputs":[]}]}