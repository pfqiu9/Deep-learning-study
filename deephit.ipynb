{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv(\"/kaggle/input/breast-test/res_data1.csv\")\ntrain, test = train_test_split(data, test_size=0.2)\n\n\ntrain[\"flag\"] = 1\ntest[\"flag\"] = 0\n## !/usr/bin/env python\n# coding: utf-8\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# For preprocessing\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn_pandas import DataFra\n\ndata = pd.concat([train, test])\n\n\nfeatures_cat = [\"hualiao\",\"xiluoda\",\"fangliao\",\"neifenmi\",\"baxiang\",\"fufa\",\"yuanchuzhuanyi\",\"zuzhixue_leixing\",\n                \"T_fenqi\",\"fenhua_xuhao\",\"linbajie_zhuangtai\",\"N_fenqi\",\"TNM_fenqi\",\"012/3\",\"HR\",\"HER-2\",\n                \"Ki67\",\"CK56\",\"EGFR\",\"fenxing_xuhao\",\"shoushu_leibie\",\"OP4\"]\nfeatures_con = [\"mm\",\"LN\",\"CCI_score\",\"age_score\",\"age-CCI_score\"]\n\ndf_dummy = pd.get_dummies(data[features_cat])\ndata = pd.concat([data, df_dummy], axis = 1)\n\ntrain = data[data[\"flag\"] == 1]\ntest = data[data[\"flag\"] == 0]\n\n\nfeatures = df_dummy.columns.to_list() + features_con\ntrain_sel = train[[\"OS_month\", \"siwang\"] + features]\ntest_sel = test[[\"OS_month\", \"siwang\"] + features]\ntrain_sel.to_csv(\"/kaggle/working/data_train.csv\", index = False)\ntest_sel.to_csv(\"data_test.csv\", index = False)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-29T00:13:14.950086Z","iopub.execute_input":"2022-08-29T00:13:14.951072Z","iopub.status.idle":"2022-08-29T00:13:16.345370Z","shell.execute_reply.started":"2022-08-29T00:13:14.950959Z","shell.execute_reply":"2022-08-29T00:13:16.344382Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"pip install pycox","metadata":{"execution":{"iopub.status.busy":"2022-08-29T00:13:16.347322Z","iopub.execute_input":"2022-08-29T00:13:16.347677Z","iopub.status.idle":"2022-08-29T00:13:37.993414Z","shell.execute_reply.started":"2022-08-29T00:13:16.347646Z","shell.execute_reply":"2022-08-29T00:13:37.991731Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pycox\n  Downloading pycox-0.2.3-py3-none-any.whl (73 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m244.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: feather-format>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from pycox) (0.4.1)\nRequirement already satisfied: scikit-learn>=0.21.2 in /opt/conda/lib/python3.7/site-packages (from pycox) (1.0.2)\nCollecting py7zr>=0.11.3\n  Downloading py7zr-0.20.0-py3-none-any.whl (64 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m869.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from pycox) (3.7.0)\nRequirement already satisfied: requests>=2.22.0 in /opt/conda/lib/python3.7/site-packages (from pycox) (2.28.1)\nCollecting torchtuples>=0.2.0\n  Downloading torchtuples-0.2.2-py3-none-any.whl (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numba>=0.44 in /opt/conda/lib/python3.7/site-packages (from pycox) (0.55.2)\nRequirement already satisfied: pyarrow>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from feather-format>=0.4.0->pycox) (8.0.0)\nRequirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from h5py>=2.9.0->pycox) (1.21.6)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba>=0.44->pycox) (59.8.0)\nRequirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /opt/conda/lib/python3.7/site-packages (from numba>=0.44->pycox) (0.38.1)\nCollecting pyzstd>=0.14.4\n  Downloading pyzstd-0.15.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (379 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.2/379.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from py7zr>=0.11.3->pycox) (5.9.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from py7zr>=0.11.3->pycox) (4.12.0)\nCollecting pycryptodomex>=3.6.6\n  Downloading pycryptodomex-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting pybcj>=0.6.0\n  Downloading pybcj-1.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting brotli>=1.0.9\n  Downloading Brotli-1.0.9-cp37-cp37m-manylinux1_x86_64.whl (357 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.2/357.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pyppmd<0.19.0,>=0.18.1\n  Downloading pyppmd-0.18.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.6/138.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting inflate64>=0.3.0\n  Downloading inflate64-0.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (92 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting multivolumefile>=0.2.3\n  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\nRequirement already satisfied: texttable in /opt/conda/lib/python3.7/site-packages (from py7zr>=0.11.3->pycox) (1.6.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.22.0->pycox) (2022.6.15)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.22.0->pycox) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.22.0->pycox) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.22.0->pycox) (1.26.11)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.2->pycox) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.2->pycox) (1.0.1)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.2->pycox) (1.7.3)\nRequirement already satisfied: pandas>=0.24.2 in /opt/conda/lib/python3.7/site-packages (from torchtuples>=0.2.0->pycox) (1.3.5)\nRequirement already satisfied: matplotlib>=3.0.3 in /opt/conda/lib/python3.7/site-packages (from torchtuples>=0.2.0->pycox) (3.5.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (1.4.3)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (4.33.3)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (2.8.2)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (3.0.9)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (9.1.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (21.3)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.2->torchtuples>=0.2.0->pycox) (2022.1)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->py7zr>=0.11.3->pycox) (4.3.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->py7zr>=0.11.3->pycox) (3.8.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (1.15.0)\nInstalling collected packages: brotli, pyzstd, pyppmd, pycryptodomex, multivolumefile, pybcj, inflate64, torchtuples, py7zr, pycox\nSuccessfully installed brotli-1.0.9 inflate64-0.3.0 multivolumefile-0.2.3 py7zr-0.20.0 pybcj-1.0.1 pycox-0.2.3 pycryptodomex-3.15.0 pyppmd-0.18.3 pyzstd-0.15.3 torchtuples-0.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# For preprocessing\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn_pandas import DataFrameMapper \n\nimport torch # For building the networks \nimport torchtuples as tt # Some useful functions\n\nfrom pycox.datasets import metabric\nfrom pycox.models import DeepHitSingle\nfrom pycox.evaluation import EvalSurv\n\n\nnp.random.seed(1234)\n_ = torch.manual_seed(123)\ndf_train = pd.read_csv(\"/kaggle/working/data_train.csv\")\ndf_test = pd.read_csv(\"/kaggle/working/data_test.csv\")\ncols_standardize = ['age_score', \"linbajie_zhuangtai\"]\ncols_leave = [x for x in df_train.columns.to_list() if x not in [\"siwang\", \"OS_month\", \"age_score\", \"linbajie_zhuangtai\"]]\nstandardize = [([col], StandardScaler()) for col in cols_standardize]\nleave = [(col, None) for col in cols_leave]\nx_mapper = DataFrameMapper(standardize + leave)\nx_train = x_mapper.fit_transform(df_train).astype('float32')\nx_test = x_mapper.transform(df_test).astype('float32')\nnum_durations = 108\nlabtrans = DeepHitSingle.label_transform(num_durations)\nget_target = lambda df: (df['siwang'].values, df['OS_month'].values)\ny_train = labtrans.fit_transform(*get_target(df_train))\ny_test = labtrans.fit_transform(*get_target(df_test))\ntrain = (x_train, y_train)\n\n# We don't need to transform the test labels\ndurations_test, events_test = get_target(df_test)\n\n\n\nin_features = x_train.shape[1]\nout_features = labtrans.out_features\n\n\nlist_num_nodes = [[32, 8], [16, 8], [16, 4], [8, 4]]\nlist_batch_norm = [False, True]\nlist_dropout = [0.0]\nlist_alpha = [0.1, 0.2, 0.3, 0.4, 0.5]\nlist_sigma = [0.1, 0.2, 0.3, 0.4, 0.5]\nlist_batch_size = [64, 128, 256, 512, 1024]\nlist_lr = [0.1, 0.01, 0.001, 0.0001]\n\nparameters = []\nfor num_nodes in list_num_nodes:\n    for batch_norm in list_batch_norm:\n        for dropout in list_dropout:\n            for alpha in list_alpha:\n                for sigma in list_sigma:\n                    for batch_size in list_batch_size:\n                        for lr in list_lr:\n                            parameters.append([num_nodes, batch_norm, dropout, alpha, sigma, batch_size, lr])\n\n\ndeephit_cv_results = pd.DataFrame(parameters)\ndeephit_cv_results[\"cindex\"] = 0\n\n\n\nkf = KFold(n_splits = 5)\n\n\nfor index in range(deephit_cv_results.shape[0]):\n    print(index)\n    num_nodes = deephit_cv_results.iloc[index, 0]\n    batch_norm = deephit_cv_results.iloc[index, 1]\n    dropout = deephit_cv_results.iloc[index, 2]\n    alpha = deephit_cv_results.iloc[index, 3]\n    sigma = deephit_cv_results.iloc[index, 4]\n    batch_size = deephit_cv_results.iloc[index, 5]\n    lr = deephit_cv_results.iloc[index, 6]\n    cindexes = []\n    for train_index, test_index in kf.split(df_train):\n        # print(\"Train:\", train_index, \"Validation:\",test_index)\n        X_tr = x_train[train_index, ]\n        X_val = x_train[test_index, ]\n        Y_tr_0 = y_train[0][train_index, ]\n        Y_tr_1 = y_train[1][train_index, ]\n        Y_val_0 = y_train[0][test_index, ]\n        Y_val_1 = y_train[1][test_index, ]\n        Y_tr = (Y_tr_0, Y_tr_1)\n        Y_val = (Y_val_0, Y_val_1)\n        net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm, dropout)\n        model = DeepHitSingle(net,\n                      tt.optim.Adam, \n                      alpha = alpha, \n                      sigma = sigma, \n                      duration_index = labtrans.cuts)\n        model.optimizer.set_lr(lr)\n        epochs = 1000\n        callbacks = [tt.callbacks.EarlyStopping(patience = 3)]\n        log = model.fit(X_tr, Y_tr, int(batch_size), epochs, callbacks, val_data = (X_val, Y_val))\n        surv = model.predict_surv_df(X_val)\n        ev = EvalSurv(surv, Y_val_0, Y_val_1, censor_surv='km')\n        c_index = ev.concordance_td('antolini')\n        cindexes.append(c_index)\n    deephit_cv_results.iloc[index, 7] = np.mean(cindexes)\n    deephit_cv_results.to_csv('/kaggle/working/cv.results.deephit.csv', index = False)\n    print(index, np.mean(cindexes))\n    \ndeephit_cv_results = pd.read_csv(\"/kaggle/working/cv.results.deephit.csv\")\nprint(deephit_cv_results[\"cindex\"].values.max())\nind_best = deephit_cv_results[\"cindex\"].values.argmax()\n# num_nodes = eval(deephit_cv_results.iloc[ind_best, 0])\nnum_nodes = deephit_cv_results.iloc[ind_best, 0]\nbatch_norm = deephit_cv_results.iloc[ind_best, 1]\ndropout = deephit_cv_results.iloc[ind_best, 2]\nalpha = deephit_cv_results.iloc[ind_best, 3]\nsigma = deephit_cv_results.iloc[ind_best, 4]\nbatch_size = deephit_cv_results.iloc[ind_best, 5]\nlr = deephit_cv_results.iloc[ind_best, 6]\n\n        \nnet = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm, dropout)\nmodel = DeepHitSingle(net,\n              tt.optim.Adam, \n              alpha = alpha, \n              sigma = sigma, \n              duration_index = labtrans.cuts)\nmodel.optimizer.set_lr(lr)\nepochs = 100\ncallbacks = [tt.callbacks.EarlyStopping(patience = 3)]\nlog = model.fit(x_train, y_train, int(batch_size), epochs, callbacks, val_data = (x_test, y_test))\n\nsurv = model.predict_surv_df(x_test)\nev = EvalSurv(surv, df_test[\"siwang\"].values, df_test[\"OS_month\"].values, censor_surv='km')\nc_index = ev.concordance_td('antolini')\nprint('C-index: {:.4f}'.format(c_index))\n\ntime_grid = np.linspace(df_test[\"siwang\"].values.min(), df_test[\"siwang\"].values.max(), 108)\nibs = ev.integrated_brier_score(time_grid) \nprint('IBS: {:.4f}'.format(ibs))\n\n\n\n\ndef bootstrap_replicate_1d(data):\n    bs_sample = np.random.choice(data,len(data))\n    return bs_sample\n\n\nbootstrap_R = 100\nc_indexes = []\nibss = []\n\n\nfor i in range(bootstrap_R):\n    print(i)\n    train_bs_idx = bootstrap_replicate_1d(np.array(range(df_train.shape[0])))\n    # Creating the X, T and E input\n    X_train = x_train[train_bs_idx, ]\n    T_train = y_train[0][train_bs_idx]\n    E_train = y_train[1][train_bs_idx]\n    Y_train = (T_train, E_train)\n    net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm, dropout)\n    model = DeepHitSingle(net,\n                  tt.optim.Adam, \n                  alpha = alpha, \n                  sigma = sigma, \n                  duration_index = labtrans.cuts)\n    model.optimizer.set_lr(lr)\n    epochs = 100\n    callbacks = [tt.callbacks.EarlyStopping(patience = 3)]\n    log = model.fit(X_train, Y_train, int(batch_size), epochs, callbacks, val_data = (x_test, y_test))\n    surv = model.predict_surv_df(x_test)\n    ev = EvalSurv(surv, df_test[\"siwang\"].values, df_test[\"OS_month\"].values, censor_surv='km')\n    # ev = EvalSurv(surv, Y_val_0, Y_val_1, censor_surv='km')\n    c_index = ev.concordance_td('antolini')\n    time_grid = np.linspace(df_test[\"siwang\"].values.min(), df_test[\"siwang\"].values.max(), 100)\n    ibs = ev.integrated_brier_score(time_grid) \n    c_indexes.append(np.round(c_index, 4))\n    ibss.append(np.round(ibs, 4))\n\n\npd.DataFrame(data = {\"cindex\": c_indexes, \"ibs\": ibss}).to_csv(\"/kaggle/working/results.ci.deephit.csv\", index=False)\n\n# Compute the 95% confidence interval: conf_int\nmean_cindex = np.mean(c_indexes)\nmean_ibs = np.mean(ibss)\n\n\n# Print the mean\nprint('mean cindex =', mean_cindex)\nprint('mean ibs =', mean_ibs)\n\n\nci_cindex = np.percentile(c_indexes, [2.5, 97.5])\nci_ibs = np.percentile(ibss, [2.5, 97.5])\n\n# Print the confidence interval\nprint('confidence interval =', ci_cindex)\nprint('confidence interval =', ci_ibs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## !/usr/bin/env python\n# coding: utf-8\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# For preprocessing\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn_pandas import DataFrameMapper \n\nimport torch # For building the networks \nimport torchtuples as tt # Some useful functions\n\nfrom pycox.datasets import metabric\nfrom pycox.models import DeepHitSingle\nfrom pycox.evaluation import EvalSurv\n\n\nnp.random.seed(1234)\n_ = torch.manual_seed(123)\n\n\n# In[3]:\n\n\ndf_train = pd.read_csv(\"/kaggle/working/data_train.csv\")\ndf_test = pd.read_csv(\"/kaggle/working/data_test.csv\")\n\n\ncols_standardize = ['age_score', \"linbajie_zhuangtai\"]\ncols_leave = [x for x in df_train.columns.to_list() if x not in [\"siwang\", \"OS_month\", \"age_score\", \"linbajie_zhuangtai\"]]\nstandardize = [([col], StandardScaler()) for col in cols_standardize]\nleave = [(col, None) for col in cols_leave]\nx_mapper = DataFrameMapper(standardize + leave)\nx_train = x_mapper.fit_transform(df_train).astype('float32')\nx_test = x_mapper.transform(df_test).astype('float32')\n\n\n\nnum_durations = 108\nlabtrans = DeepHitSingle.label_transform(num_durations)\nget_target = lambda df: (df['siwang'].values, df['OS_month'].values)\ny_train = labtrans.fit_transform(*get_target(df_train))\ny_test = labtrans.fit_transform(*get_target(df_test))\ntrain = (x_train, y_train)\n\n# We don't need to transform the test labels\ndurations_test, events_test = get_target(df_test)\n\n\n\nin_features = x_train.shape[1]\nout_features = labtrans.out_features\n\n\nlist_num_nodes = [[32, 8], [16, 8], [16, 4], [8, 4]]\nlist_batch_norm = [False, True]\nlist_dropout = [0.0]\nlist_alpha = [0.1, 0.2, 0.3, 0.4, 0.5]\nlist_sigma = [0.1, 0.2, 0.3, 0.4, 0.5]\nlist_batch_size = [64, 128, 256, 512, 1024]\nlist_lr = [0.1, 0.01, 0.001, 0.0001]\n\nparameters = []\nfor num_nodes in list_num_nodes:\n    for batch_norm in list_batch_norm:\n        for dropout in list_dropout:\n            for alpha in list_alpha:\n                for sigma in list_sigma:\n                    for batch_size in list_batch_size:\n                        for lr in list_lr:\n                            parameters.append([num_nodes, batch_norm, dropout, alpha, sigma, batch_size, lr])\n\n\ndeephit_cv_results = pd.DataFrame(parameters)\ndeephit_cv_results[\"cindex\"] = 0\n\n\n\nkf = KFold(n_splits = 5)\n\n\nfor index in range(deephit_cv_results.shape[0]):\n    print(index)\n#     num_nodes = eval(deephit_cv_results.iloc[index, 0])\n    num_nodes = deephit_cv_results.iloc[index, 0]\n    batch_norm = deephit_cv_results.iloc[index, 1]\n    dropout = deephit_cv_results.iloc[index, 2]\n    alpha = deephit_cv_results.iloc[index, 3]\n    sigma = deephit_cv_results.iloc[index, 4]\n    batch_size = deephit_cv_results.iloc[index, 5]\n    lr = deephit_cv_results.iloc[index, 6]\n    cindexes = []\n    for train_index, test_index in kf.split(df_train):\n        # print(\"Train:\", train_index, \"Validation:\",test_index)\n        X_tr = x_train[train_index, ]\n        X_val = x_train[test_index, ]\n        Y_tr_0 = y_train[0][train_index, ]\n        Y_tr_1 = y_train[1][train_index, ]\n        Y_val_0 = y_train[0][test_index, ]\n        Y_val_1 = y_train[1][test_index, ]\n        Y_tr = (Y_tr_0, Y_tr_1)\n        Y_val = (Y_val_0, Y_val_1)\n        net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm, dropout)\n        model = DeepHitSingle(net,\n                      tt.optim.Adam, \n                      alpha = alpha, \n                      sigma = sigma, \n                      duration_index = labtrans.cuts)\n        model.optimizer.set_lr(lr)\n        epochs = 1000\n        callbacks = [tt.callbacks.EarlyStopping(patience = 3)]\n        log = model.fit(X_tr, Y_tr, int(batch_size), epochs, callbacks, val_data = (X_val, Y_val))\n        surv = model.predict_surv_df(X_val)\n        ev = EvalSurv(surv, Y_val_0, Y_val_1, censor_surv='km')\n        c_index = ev.concordance_td('antolini')\n        cindexes.append(c_index)\n    deephit_cv_results.iloc[index, 7] = np.mean(cindexes)\n    deephit_cv_results.to_csv('/kaggle/working/cv.results.deephit.csv', index = False)\n    print(index, np.mean(cindexes))\n\n\ndeephit_cv_results = pd.read_csv(\"/kaggle/working/cv.results.deephit.csv\")\nprint(deephit_cv_results[\"cindex\"].values.max())\nind_best = deephit_cv_results[\"cindex\"].values.argmax()\n# num_nodes = eval(deephit_cv_results.iloc[ind_best, 0])\nnum_nodes = deephit_cv_results.iloc[ind_best, 0]\nbatch_norm = deephit_cv_results.iloc[ind_best, 1]\ndropout = deephit_cv_results.iloc[ind_best, 2]\nalpha = deephit_cv_results.iloc[ind_best, 3]\nsigma = deephit_cv_results.iloc[ind_best, 4]\nbatch_size = deephit_cv_results.iloc[ind_best, 5]\nlr = deephit_cv_results.iloc[ind_best, 6]\n\n        \nnet = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm, dropout)\nmodel = DeepHitSingle(net,\n              tt.optim.Adam, \n              alpha = alpha, \n              sigma = sigma, \n              duration_index = labtrans.cuts)\nmodel.optimizer.set_lr(lr)\nepochs = 100\ncallbacks = [tt.callbacks.EarlyStopping(patience = 3)]\nlog = model.fit(x_train, y_train, int(batch_size), epochs, callbacks, val_data = (x_test, y_test))\n\nsurv = model.predict_surv_df(x_test)\nev = EvalSurv(surv, df_test[\"siwang\"].values, df_test[\"OS_month\"].values, censor_surv='km')\nc_index = ev.concordance_td('antolini')\nprint('C-index: {:.4f}'.format(c_index))\n\ntime_grid = np.linspace(df_test[\"siwang\"].values.min(), df_test[\"siwang\"].values.max(), 108)\nibs = ev.integrated_brier_score(time_grid) \nprint('IBS: {:.4f}'.format(ibs))\n\n\n\n\ndef bootstrap_replicate_1d(data):\n    bs_sample = np.random.choice(data,len(data))\n    return bs_sample\n\n\nbootstrap_R = 100\nc_indexes = []\nibss = []\n\n\nfor i in range(bootstrap_R):\n    print(i)\n    train_bs_idx = bootstrap_replicate_1d(np.array(range(df_train.shape[0])))\n    # Creating the X, T and E input\n    X_train = x_train[train_bs_idx, ]\n    T_train = y_train[0][train_bs_idx]\n    E_train = y_train[1][train_bs_idx]\n    Y_train = (T_train, E_train)\n    net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm, dropout)\n    model = DeepHitSingle(net,\n                  tt.optim.Adam, \n                  alpha = alpha, \n                  sigma = sigma, \n                  duration_index = labtrans.cuts)\n    model.optimizer.set_lr(lr)\n    epochs = 100\n    callbacks = [tt.callbacks.EarlyStopping(patience = 3)]\n    log = model.fit(X_train, Y_train, int(batch_size), epochs, callbacks, val_data = (x_test, y_test))\n    surv = model.predict_surv_df(x_test)\n    ev = EvalSurv(surv, df_test[\"siwang\"].values, df_test[\"OS_month\"].values, censor_surv='km')\n    # ev = EvalSurv(surv, Y_val_0, Y_val_1, censor_surv='km')\n    c_index = ev.concordance_td('antolini')\n    time_grid = np.linspace(df_test[\"siwang\"].values.min(), df_test[\"siwang\"].values.max(), 100)\n    ibs = ev.integrated_brier_score(time_grid) \n    c_indexes.append(np.round(c_index, 4))\n    ibss.append(np.round(ibs, 4))\n\n\npd.DataFrame(data = {\"cindex\": c_indexes, \"ibs\": ibss}).to_csv(\"/kaggle/working/results.ci.deephit.csv\", index=False)\n\n# Compute the 95% confidence interval: conf_int\nmean_cindex = np.mean(c_indexes)\nmean_ibs = np.mean(ibss)\n\n\n# Print the mean\nprint('mean cindex =', mean_cindex)\nprint('mean ibs =', mean_ibs)\n\n\nci_cindex = np.percentile(c_indexes, [2.5, 97.5])\nci_ibs = np.percentile(ibss, [2.5, 97.5])\n\n# Print the confidence interval\nprint('confidence interval =', ci_cindex)\nprint('confidence interval =', ci_ibs)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-28T10:55:14.444955Z","iopub.execute_input":"2022-08-28T10:55:14.445520Z"}}}]}